{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " MNIST_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "84YyQfoOtVPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 使用1.x版本的tensorflow\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 讀取mnist資料 \n",
        "# x_train為60000筆訓練資料,每一筆資料為28x28的灰階手寫數字\n",
        "# y_train為60000筆資料的正確數字(0,1,2,3,4,5,6,7,8,9)\n",
        "# x_test為10000筆訓練資料,每一筆資料為28x28的灰階手寫數字\n",
        "# y_test為10000筆資料的正確數字(0,1,2,3,4,5,6,7,8,9)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"x_train.shape={}, y_train.shape={}\".format(x_train.shape, y_train.shape))\n",
        "print(\"x_test.shape={}, y_test.shape={}\".format(x_test.shape, y_test.shape))\n",
        "\n",
        "\n",
        "# 接著我們必須將x_train及x_test dataset由原本三維轉為四維矩陣以符合CNN的需求\n",
        "# 這是因為RGB圖片的格式為為width, height, channels，加上ID數維度為4。\n",
        "# MNIST圖片為灰階因此其channel為1，轉換後的shape為(ID, width, height, channel)\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "print(\"x_train.shape={}\".format(x_train.shape))\n",
        "print(\"x_test.shape={}\".format(x_test.shape))\n",
        "\n",
        "\n",
        "# 接下來將dataset的特徵值進行標準化，方法是除以255（因為圖像的像素點介於0~255之間）\n",
        "# 可讓所有的特徵值介於0與1之間。除了可提昇模型預測的準確度，梯度運算時也能更快收斂。\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "\n",
        "# 手寫數字結果如果採用原來的1, 2, 3....在演算法中，會以為2是1的兩倍大，3是1的三倍大，其實並沒有這個意義\n",
        "# 因此會採用Onehot encoding，將彼此間不相關且非有序的categories轉換為連續性且是有序數值。\n",
        "# 舉例來說，原本y_train[0]=5，經過onehot encoding會變成[0,0,0,0,0,1,0,0,0,0]\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test_categories = y_test\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# CNN模型建置\n",
        "# filter代表過濾器的數量, kernel_size代表filter的大小\n",
        "# padding: filter遇到邊緣的處理模式('same'為補0, 'valid'為忽略)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=(5, 5), padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(filters=36, kernel_size=(5, 5), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()     \n",
        "\n",
        "\n",
        "# 模型編譯\n",
        "# loss:損失函數 optimizer:學習函數(梯度縮減的方式) metrics:評估模型好壞的方式(accuracy=正確率)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 模型訓練\n",
        "# x:訓練資料 y:訓練資料的答案 validation_split:訓練資料的多少百分比要拿來驗證 validation_data:驗證資料 \n",
        "# epochs:模型訓練次數 \n",
        "\n",
        "train_history=model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), validation_split=0.2, epochs=10, batch_size=300, verbose=2)\n",
        "\n",
        "\n",
        "# 模型評估\n",
        "\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"模型正確率為：{}\".format(scores[1]))\n",
        "\n",
        "\n",
        "#繪製正確性與損失立歷史圖\n",
        "def show_train_history(train_history, train, validation):\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Train History')\n",
        "    plt.ylabel('Train')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['train', 'validation'], loc='center right')\n",
        "    plt.show()\n",
        "\n",
        "show_train_history(train_history, 'accuracy', 'val_accuracy')\n",
        "show_train_history(train_history, 'loss', 'val_loss')\n",
        "\n",
        "# Confusion Matrix混淆矩陣\n",
        "import pandas as pd\n",
        "prediction = model.predict_classes(x_test)\n",
        "print(y_test.shape)\n",
        "pd.crosstab(y_test_categories, prediction, rownames=['label'], colnames=['predict'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}